{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab #1\n",
    "\n",
    "TA: Matt Ploenzke\n",
    "Date: 4/5/2019\n",
    "\n",
    "Today's lab consists of practice questions to review the topics presented thus far in class. We will be focusing on:\n",
    "    1. Neural network terminology and architecture\n",
    "    2. python\n",
    "    3. Forward and backward propagation\n",
    "    4. Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Let's review the terminology introduced by thinking about how to design a model for each the following scenarios. It's important to remember that while there is more than one correct answer in these cases, we want to develop an intuition to help save time in parameter tuning, training, computational resources, etc. We'll also briefy touch on some advanced topics to provide a foundation for later in the course, and remember you do not need to use a deep neural network in every case.\n",
    "\n",
    "*Case 1:* The input is the MNIST handwritten digits dataset (features are 28x28 pixel intensities and labels are digits 0-9). We want to predict which digit the image represents and there are only 10 images per category ($N=100$).\n",
    "\n",
    "    - Random forest, k-nearest neighbors because of the small sample size and relatively easy prediction task.\n",
    "\n",
    "*Case 2:* The identical setup but this time there are thousands of images per category.\n",
    "\n",
    "    - Either of the above methods are fine, or using a simple neural network. Activation function needs to be softmax and loss function needs to be categorical cross-entropy.\n",
    "\n",
    "*Case 3:* The identical setup as case 2 but this time images may contain multiple digits or no digits at all.\n",
    "\n",
    "    - Last layer activation should be sigmoid and BCE.\n",
    "\n",
    "*Case 4:*  The covariates are BMI measurements and reported smoking status, the labels are binary denoting cardiovascular disease. Our sample consists of 70 individuals and we want to predict an individuals' health status based on their BMI and smoking status. We are interested in the effect of BMI on cardiovascular disease.\n",
    "\n",
    "    - Logistic regression.\n",
    "\n",
    "*Case 5:* The input consists of thousands of images of different animals and we want to classify which animal the image contains. \n",
    "\n",
    "    - CNN with softmax and CCE or sigmoid and BCE.\n",
    "\n",
    "*Case 6:* The input consists of thousands of English sentences and we want to predict the next word in the sentences. \n",
    "\n",
    "    - RNN\n",
    "\n",
    "*Case 7:* The input consists of biomarker status for thousands of loci across thousands of individuals (i.e. Ancestry.com). There are no associated labels and we wish to learn about population substructure. \n",
    "\n",
    "    - PCA, VAE, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "a) Start a jupyter notebook and/or create a python file\n",
    "\n",
    "b) Install and load numpy. What is your package manager?\n",
    "\n",
    "c) If you don't have keras and tensorflow installed, install those now. \n",
    "\n",
    "d) Ask Matt any questions about python now or forever hold your peace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Draw the architecture of a neural network satisying the following conditions:\n",
    "    1. X is a univariate covariate. We will consider the case when X=5.\n",
    "    2. There are two hidden layers. The first consists of two nodes, each with a bias term taking values (-1 and 1, respectively). The weight going to the first node takes value 0.5 and the weight going to the second node takes value -0.5.\n",
    "    3. The nodes in hidden layer 1 each use a linear activation function.\n",
    "    4. Hidden layer 2 consists of a single node with no bias term and the ReLU activation function. The weight from node 1 in hidden layer 1 is 0.3 and the weight from node 2 in hidden layer 1 is 0.7.\n",
    "    5. Hidden layer 2 outputs to a single output node. The bias term for the output node is 0.5 and the weight from hidden layer 2 is 2. \n",
    "    6. The loss function to be optimized is squared loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Lab1_q3.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Implement a single forward pass of the network described in Question 3. You do not need to implement the network in keras and should instead use numpy operations (either scalar or matrix). Start by defining the weights and input matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 5]) # add bias/intercept as first entry\n",
    "w_hidden1 = np.matrix([[-1, 1], [.5, -.5]]) # 2x2 matrix of first-layer biases and weights\n",
    "w_hidden2 = np.matrix([[.3], [.7]]) # 1x2 matrix of second-layer weights\n",
    "w_out = 2 # 1x1 scalar of third-layer weights\n",
    "b_out = 0.5 # 1x1 scalar of third-layer bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = np.matmul(x,w_hidden1) # perform matrix multiplication to get hidden layer 1\n",
    "hidden2 = np.matmul(hidden1,w_hidden2) # perform matrix multiplication to get hidden layer 2\n",
    "hidden2_clamped = np.maximum(hidden2, 0) # relu\n",
    "y_hat = hidden2_clamped*w_out + b_out # perform third multiplication to get output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's print the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values for the hidden layer 1 are: [[ 1.5 -1.5]]\n",
      "The values for the hidden layer 2 are: [[-0.6]]\n",
      "The post-relu values for the hidden layer 2 are: [[0.]]\n",
      "The value for the output layer is: [[0.5]]\n"
     ]
    }
   ],
   "source": [
    "print('The values for the hidden layer 1 are:', hidden1)\n",
    "print('The values for the hidden layer 2 are:', hidden2)\n",
    "print('The post-relu values for the hidden layer 2 are:', hidden2_clamped)\n",
    "print('The value for the output layer is:', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the loss for the training example given a label of Y=.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is: [[0.0625]]\n"
     ]
    }
   ],
   "source": [
    "y_i = .25 # positive outcome as defined in the problem\n",
    "loss_i = (y_i-y_hat)**2\n",
    "print('The loss is:',loss_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a single backward pass of the network. Again use numpy. Start by defining the individual gradient terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient for loss\n",
    "dl_dy = 2*(y_i-y_hat) # gradient of loss wrt predicted probability (1x1)\n",
    "\n",
    "# gradients for output layer\n",
    "dy_dhidden2_clamped = w_out # gradient of y_hat wrt hidden output (1x1)\n",
    "dy_dw_out = hidden2_clamped # gradient of  y_hat wrt output layer weight (1x1)\n",
    "dy_db_out = 1 # gradient of  y_hat wrt output layer bias (1x1)\n",
    "\n",
    "# gradient (gate) for relu\n",
    "dhidden2_clamped_dhidden2 = (hidden2>0)*1 # gradient for relu\n",
    "\n",
    "# gradients for second hidden layer\n",
    "dhidden2_dw_2 = hidden1 # gradient of second hidden layer wrt second hidden layer weights\n",
    "dhidden2_dhidden1 = w_hidden2 # gradient of second hidden layer wrt first hidden layer output\n",
    "\n",
    "# gradients for first hidden layer\n",
    "dhidden1_dw_1 = x # gradient of first hidden layer wrt first hidden layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dw_out = dl_dy*dy_dw_out # gradient of loss wrt output weights\n",
    "dl_db_out = dl_dy*1 # gradient of loss wrt output bias\n",
    "dl_dw_2 = dl_dy*dy_dhidden2_clamped*dhidden2_clamped_dhidden2*dhidden2_dw_2 # gradient of loss wrt second hidden layer weights\n",
    "dl_dw_11 = dl_dy*dy_dhidden2_clamped*dhidden2_clamped_dhidden2*dhidden2_dhidden1[0]*dhidden1_dw_1 # gradient of loss wrt first hidden layer weights (node 1)\n",
    "dl_dw_12 = dl_dy*dy_dhidden2_clamped*dhidden2_clamped_dhidden2*dhidden2_dhidden1[1]*dhidden1_dw_1 # gradient of loss wrt first hidden layer weights (node 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a maximum of two quantities are needed for each layer: \n",
    "    \n",
    "    1) the partial derivative w.r.t. the input\n",
    "    2) the partial derivative w.r.t. the weight/parameter\n",
    "    \n",
    "What is the purpose of each of these quantities?\n",
    "\n",
    "    - The partial derivative w.r.t. the input is used in the backprop/chain-rule calculations.\n",
    "    - The partial derivative w.r.t. the parameter/weights is used to update the parameter values in the training loop via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Load the MNIST dataset provided by keras. This contains 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reshape the data to fit the keras format. Don't worry too much about this chunk for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the shape again to see what changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 in Homework 1 asks you to train a neural network on the Boston housing data. This dataset contains features on very different scales (for example there are both binary features and real-valued features). While the MNIST features take on values between 0 and 1 and do not need to be normalized, we will go through the exercise of normalizing the values before training our network.\n",
    "\n",
    "Can you think of other algorithms in which normalization is necessary? Is it necessary in the case of neural networks? Why or why not? \n",
    "\n",
    "    - Clustering, PCA, random forest, etc. Not necessary (universal function approximator) but makes training easier in cases in which the features have very different scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Be sure to normalize the test set with the training set mean and standard deviation. Don't forget to convert the training and testing sets to float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "mean = x_train.mean()\n",
    "x_train -= mean\n",
    "std = x_train.std()\n",
    "x_train /= std\n",
    "\n",
    "x_test -= mean\n",
    "x_test /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will the code above need to be changed for Boston housing dataset? Why?\n",
    "\n",
    "    - Need to calculate mean and standard deviation per feature, thus need to use something like x_train.mean(axis=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define and fit our model let's one-hot encode the labels. Don't forget to do the same for the testing labels and note you will not need to do this step in the case of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a shallow convolutional neural network with a single dense layer. Include 32 convolutional filters of size 3x3 and use the relu activation function.\n",
    "\n",
    "After the convolutional layer, flatted the tensor to be fed into the dense layer.\n",
    "\n",
    "In the dense layer use enough output nodes to have one corresponding to each class label (10). What is the activation function you should use here?\n",
    "\n",
    "In the optimizer use the `Adadelta` optimization function, and choose an appropriate loss function and model performance measure. \n",
    "\n",
    "Run the network for 5 epochs and use a batch_size of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 16s 266us/step - loss: 8.1845 - acc: 0.4809\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.1419 - acc: 0.9637\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.0530 - acc: 0.9849\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 0.0298 - acc: 0.9920\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 0.0170 - acc: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f844cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the test set accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
